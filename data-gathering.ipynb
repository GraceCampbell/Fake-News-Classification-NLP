{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit Classification with Natural Language Processing\n",
    "\n",
    "## Data Gathering\n",
    "\n",
    "*Author: Grace Campbell*\n",
    "\n",
    "#### Project Directory\n",
    "1. Data Preparation \n",
    "    - *Data Gathering*\n",
    "    - [Exploratory Data Analysis](https://github.com/GraceCampbell/Fake-News-Classification-NLP/blob/master/exploratory-data-analysis.ipynb)\n",
    "2. Modeling\n",
    "    - [Naive Bayes](https://github.com/GraceCampbell/Fake-News-Classification-NLP/blob/master/modeling-naive-bayes.ipynb)\n",
    "    - [$k$-Nearest Neighbors](https://github.com/GraceCampbell/Fake-News-Classification-NLP/blob/master/modeling-knn.ipynb)\n",
    "    - [Support-Vector Machine](https://github.com/GraceCampbell/Fake-News-Classification-NLP/blob/master/modeling-svm.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-agent': 'Grace'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting /r/TheOnion post titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Web scraping Reddit's API for titles of 1000 hot posts from /r/TheOnion\n",
    "onion_posts = []\n",
    "after = None\n",
    "for i in range(40):\n",
    "    if after == None:\n",
    "        params = {}\n",
    "    else:\n",
    "        params = {'after': after}\n",
    "    url = 'https://www.reddit.com/r/theonion.json'\n",
    "    res = requests.get(url, params=params, headers=headers)\n",
    "    if res.status_code == 200:\n",
    "        the_json = res.json()\n",
    "        onion_posts.extend(the_json['data']['children'])\n",
    "        after = the_json['data']['after']\n",
    "    else:\n",
    "        print(res.status_code)\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for i in range(len(onion_posts)):\n",
    "    titles.append(onion_posts[i]['data']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onion_titles = list((set(titles)))\n",
    "len(onion_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting /r/News post titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping Reddit's API for titles of 1000 hot posts from /r/News\n",
    "news_posts = []\n",
    "after = None\n",
    "for i in range(40):\n",
    "    if after == None:\n",
    "        params = {}\n",
    "    else:\n",
    "        params = {'after': after}\n",
    "    url = 'https://www.reddit.com/r/worldnews.json'\n",
    "    res = requests.get(url, params=params, headers=headers)\n",
    "    if res.status_code == 200:\n",
    "        the_json = res.json()\n",
    "        news_posts.extend(the_json['data']['children'])\n",
    "        after = the_json['data']['after']\n",
    "    else:\n",
    "        print(res.status_code)\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for i in range(len(news_posts)):\n",
    "    titles.append(news_posts[i]['data']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_titles = list(set(titles))\n",
    "len(news_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataframe of titles and their respective class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onion = pd.DataFrame(onion_titles)\n",
    "onion['is_onion'] = 1\n",
    "\n",
    "news = pd.DataFrame(news_titles)\n",
    "news['is_onion'] = 0\n",
    "\n",
    "titles = news.append(onion, ignore_index=True)\n",
    "titles.rename({0: 'title'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving dataframe to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "titles.to_csv('titles.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
