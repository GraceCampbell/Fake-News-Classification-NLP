{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit Classification with Natural Language Processing\n",
    "\n",
    "## Modeling with Naive Bayes\n",
    "\n",
    "*Author: Grace Campbell*\n",
    "\n",
    "#### Project Directory\n",
    "1. Data Preparation \n",
    "    - [Data Gathering](https://github.com/GraceCampbell/Fake-News-Classification-NLP/blob/master/data-gathering.ipynb)\n",
    "    - [Exploratory Data Analysis](https://github.com/GraceCampbell/Fake-News-Classification-NLP/blob/master/exploratory-data-analysis.ipynb)\n",
    "2. Modeling\n",
    "    - *Naive Bayes*\n",
    "    - [$k$-Nearest Neighbors](https://github.com/GraceCampbell/Fake-News-Classification-NLP/blob/master/modeling-knn.ipynb)\n",
    "    - [Support-Vector Machine](https://github.com/GraceCampbell/Fake-News-Classification-NLP/blob/master/modeling-svm.ipynb)\n",
    "    - [Final Testing on New Data](https://github.com/GraceCampbell/Fake-News-Classification-NLP/blob/master/final-models-testing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Introduction\n",
    "\n",
    "In this notebook, I will be modeling with `MultinomialNB`, which is a Naive Bayes classifier. Naive Bayes (as in Bayes' theorem) is a conditional probability model that assumes independence between the feature variables. While this is not a realistic assumption for a natural language model, the Naive Bayes classifier can make predictions with surprising accuracy.\n",
    "\n",
    "\n",
    "### Modeling Strategy\n",
    "Before I can begin modeling, I need to turn my text data into numeric data using `CountVectorizer`. This transformer will create a matrix of values, where the columns represent every word that appears in the corpus, and the rows represent each document in the corpus. The values are gross counts of how many times a word appears in a document.\n",
    "\n",
    "Both of these methods have hyperparameters that can be tuned to optimize model performance, so I will perform a grid search using a pipeline with `CountVectorizer` and `MultinomialNB` to find the best parameters for both in the context of one another.\n",
    "\n",
    "The grid search will test 3 different `CountVectorizer` hyperparameters:\n",
    "1. `max_features`: how many features to extract (chosen by highest total frequency)\n",
    "2. `min_df`: the minimum number of documents in which a feature must appear\n",
    "3. `max_df`: the maximum percentage of documents in which a feature can appear\n",
    "\n",
    "and 2 different `MultinomialNB` hyperparameters:\n",
    "1. `alpha`: the additive smoothing (Laplace/Lidstone smoothing) parameter to be used on each feature\n",
    "2. `fit_prior`: whether the model will learn the prior probabilities of the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Searching for Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tokenizer import token_func\n",
    "\n",
    "df = pd.read_csv('./materials/titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y\n",
    "X = df['title']\n",
    "y = df['is_onion']\n",
    "\n",
    "# Train-test splitting (with stratification)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a pipeline\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(tokenizer=token_func)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Hyperparameters to search over\n",
    "params = {\n",
    "    'cvec__max_features': [None, 1000],\n",
    "    'cvec__min_df': [1, 2],\n",
    "    'cvec__max_df': [0.9, 1.0],\n",
    "    'mnb__alpha': [1, 5],\n",
    "    'mnb__fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "# Fitting the grid search\n",
    "grid = GridSearchCV(pipe, params, cv=3)\n",
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8407750631844987"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': None,\n",
       " 'cvec__min_df': 1,\n",
       " 'mnb__alpha': 1,\n",
       " 'mnb__fit_prior': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which parameters did the grid search choose?\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `CountVectorizer`, the grid search decided:\n",
    "- `max_features` should be None\n",
    "    - I will have to investigate how many features are kept in the model when there is no maximum. I do not want more features than I have rows in `X_train` (to prevent collinearity), so I may have to set the `max_features` anyway, regardless of this grid search result.\n",
    "\n",
    "\n",
    "- `min_df` should be 1, effectively meaning there is no minimum document frequency\n",
    "    - Again, I need to see how many features the model keeps, and may need to change `min_df` anyway.\n",
    "\n",
    "\n",
    "- `max_df` should be 0.9, meaning a feature will not be included in the model if it appears in more than 90% of the documents\n",
    "    - Since I eliminated stopwords from the tokens, there most likely will not be many (if any) words that show up in more than 90% of the titles.\n",
    "    \n",
    "For `MultinomialNB`, the grid search decided:\n",
    "- `alpha` should be 1\n",
    "    - This will add a smoothing parameter of 1 to each feature in the data\n",
    "    \n",
    "    \n",
    "- `fit_prior` should be True\n",
    "    - The model will learn the class prior probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming `X` Using Best Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I suspected, there are almost 3 times more features in `X_train` than there are rows when `max_features` is None. For the final transformation, I will set `max_features` equal to 1187 (the number of rows) so the number of features does not exceed the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(tokenizer=token_func, max_features=1187, min_df=1, max_df=0.9)\n",
    "\n",
    "cvec.fit(X_train)\n",
    "\n",
    "X_train_c = pd.DataFrame(cvec.transform(X_train).todense(), columns=cvec.get_feature_names())\n",
    "X_test_c  = pd.DataFrame(cvec.transform(X_test).todense(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating and fitting the model\n",
    "mnb = MultinomialNB(alpha=1, fit_prior=True)\n",
    "mnb.fit(X_train_c, y_train)\n",
    "\n",
    "# Storing predictions\n",
    "y_pred = mnb.predict(X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8661616161616161"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score\n",
    "mnb.score(X_test_c, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 201\n",
      "False Negatives: 36\n",
      "True Negatives: 142\n",
      "False Positives: 17\n",
      "\n",
      "Sensitivity: 0.8481012658227848\n",
      "Specificity: 0.8930817610062893\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix + other metrics\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print(f'True Positives: {tp}')\n",
    "print(f'False Negatives: {fn}')\n",
    "print(f'True Negatives: {tn}')\n",
    "print(f'False Positives: {fp}\\n')\n",
    "print(f'Sensitivity: {tp/(tp+fn)}')\n",
    "print(f'Specificity: {tn/(tn+fp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search found that `max_features` should be None to optimize the model's performance. However, without a feature limit, the model had ~4000 features after vectorization. I had to reduce `max_features` to 1187, the number of rows in `X_train`, to reduce collinearity. To get the highest possible accuracy score from this model, I would need to gather more data so that the model could use more features to make predictions.\n",
    "\n",
    "The baseline accuracy score for the data is the score I would get if I predicted the majority class for every data point. The majority class here, /r/TheOnion, holds around 60% of the data. If I were to predict that every document in the data belonged to /r/TheOnion, I would get an accuracy score of 60%. That is to say, if a model does not predict subreddit membership with greater than 60% accuracy, then it is not a very good model.\n",
    "\n",
    "The model's accuracy score is 86.6%, which is well above the baseline score of 60%. This means that the model correctly predicted the class 86.6% of the time. The model has relatively high sensitivity at 84.8%, meaning that 84.8% of the posts that were actually from /r/TheOnion were correctly predicted to be from /r/TheOnion. The model has higher specificity, 89.3%, which means that 89.3% of posts that belong to /r/News were correctly predicted to be from /r/News.\n",
    "\n",
    "In a real-world application, it is equally important to me that this model be able to correctly predict when a post is satirical **and** when it is real. The positive class in this case is does not hold more weight than the negative class, therefore I would rather the model be very accurate than very sensitive or very specific. This model, however, is all three, which means that for my purposes it is a great model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
